{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pruthviraj3196/capstone-project---3--Credit-Card-Default-Prediction/blob/main/Copy_of_Sample_ML_Submission_Template_Credit_Card_Default_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Credit Card Default Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name  -** Pruthviraj Gopinath Barbole\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Credit Card Default Prediction Classification ML Project aims to develop a machine learning model that accurately predicts the likelihood of credit card holders defaulting on their payments. The project addresses the crucial need for financial institutions to identify high-risk customers and take proactive measures to mitigate potential losses.\n",
        "\n",
        "The project utilizes historical credit card data, including various features such as demographics, credit limit, payment history, and transaction details, to train and evaluate the predictive model. By leveraging machine learning algorithms and techniques, the project team aims to build a robust and accurate classification model capable of distinguishing between customers who are likely to default and those who are not.\n",
        "\n",
        "The project follows a structured approach, including the following key steps:\n",
        "\n",
        "- Data Collection and Exploration: Gathering relevant credit card data from various sources and performing exploratory data analysis (EDA) to gain insights into the dataset.\n",
        "\n",
        "- Data Preprocessing and Feature Engineering: Preparing the data for modeling by cleaning, transforming, and normalizing it. Feature engineering techniques are applied to derive new meaningful features that could enhance the predictive power of the model.\n",
        "\n",
        "- Model Selection and Training: Selecting appropriate machine learning algorithms for classification, such as logistic regression, decision trees, random forests, or gradient boosting. Multiple models are explored and their performance is evaluated using suitable evaluation metrics like accuracy, precision, recall, and F1-score. The models are trained on the labeled dataset and fine-tuned using techniques like cross-validation or grid search.\n",
        "\n",
        "- Model Evaluation and Validation: Assessing the performance of the trained models on a separate validation dataset to ensure generalizability and selecting the model which the best results.\n",
        "\n",
        "The project aims to deliver a credit card default prediction model that provides financial institutions with a valuable tool to make informed decisions and allocate resources more efficiently. By identifying high-risk customers beforehand, banks can take preventive actions, such as offering credit counseling, adjusting credit limits, or initiating collection processes, ultimately reducing the overall credit risk and potential financial losses.\n",
        "\n",
        "Overall, the Credit Card Default Prediction Classification ML Project aims to contribute to the advancement of the financial industry by harnessing the power of machine learning to improve credit risk assessment and decision-making processes."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Context**\n",
        "\n",
        "This project is aimed at predicting the case of customers default payments in Taiwan. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. We can use the K-S chart to evaluate which customers will default on their credit card payments"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required. \n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits. \n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule. \n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n"
      ],
      "metadata": {
        "id": "EgqfAMPPIZMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "RjUAKDcpIhTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit = pd.read_excel(\"/content/drive/MyDrive/data/default of credit card clients.xls\", header = 1)"
      ],
      "metadata": {
        "id": "7Fy4zr0vIiB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.head()"
      ],
      "metadata": {
        "id": "GfaC3beRI7Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.shape"
      ],
      "metadata": {
        "id": "be6y4nvoI_6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.info()"
      ],
      "metadata": {
        "id": "XlKddbSiJgYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.duplicated().sum()"
      ],
      "metadata": {
        "id": "9fdMB0YfJsmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.isnull().sum()"
      ],
      "metadata": {
        "id": "S8KzIlMWJ28v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df_credit.isna(), cbar=False)"
      ],
      "metadata": {
        "id": "KtSeWvana6la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- This Dataset contains 30000 lines and 25 columns.\n",
        "- default payment next month is our target variable we need to focus on this\n",
        "- There is no Missing value In the Dataset.\n",
        "- From above we can see that there is no Duplicated Value in the dataset.\n",
        "- From Dataset info we came to know that there is interger data type value in each column"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.columns"
      ],
      "metadata": {
        "id": "4qSJfq8waqjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.describe().T"
      ],
      "metadata": {
        "id": "PAORoITlau9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ID: ID of each client\n",
        "- LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit)\n",
        "- SEX: Gender (1=male, 2=female)\n",
        "- EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)\n",
        "- MARRIAGE: Marital status (1=married, 2=single, 3=others)\n",
        "- AGE: Age in years\n",
        "- PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, … 8=payment delay for eight months, 9=payment delay for nine months and above)\n",
        "- PAY_2: Repayment status in August, 2005 (scale same as above)\n",
        "- PAY_3: Repayment status in July, 2005 (scale same as above)\n",
        "- PAY_4: Repayment status in June, 2005 (scale same as above)\n",
        "- PAY_5: Repayment status in May, 2005 (scale same as above)\n",
        "- PAY_6: Repayment status in April, 2005 (scale same as above)\n",
        "- BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)\n",
        "- BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)\n",
        "- BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)\n",
        "- BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)\n",
        "- BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)\n",
        "- BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)\n",
        "- PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)\n",
        "- PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)\n",
        "- PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)\n",
        "- PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)\n",
        "- PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)\n",
        "- PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)\n",
        "- default.payment.next.month: Default payment (1=yes, 0=no)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.nunique()"
      ],
      "metadata": {
        "id": "dK9oMpLobsbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# renaming all columns"
      ],
      "metadata": {
        "id": "0f5fGDGycH_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.rename(columns={'PAY_0':'PAY_SEPT','PAY_2':'PAY_AUG','PAY_3':'PAY_JUL','PAY_4':'PAY_JUN','PAY_5':'PAY_MAY','PAY_6':'PAY_APR'},inplace=True)\n",
        "df_credit.rename(columns={'BILL_AMT1':'BILL_AMT_SEPT','BILL_AMT2':'BILL_AMT_AUG','BILL_AMT3':'BILL_AMT_JUL','BILL_AMT4':'BILL_AMT_JUN','BILL_AMT5':'BILL_AMT_MAY','BILL_AMT6':'BILL_AMT_APR'}, inplace = True)\n",
        "df_credit.rename(columns={'PAY_AMT1':'PAY_AMT_SEPT','PAY_AMT2':'PAY_AMT_AUG','PAY_AMT3':'PAY_AMT_JUL','PAY_AMT4':'PAY_AMT_JUN','PAY_AMT5':'PAY_AMT_MAY','PAY_AMT6':'PAY_AMT_APR'},inplace=True)\n",
        "df_credit.rename(columns={'default payment next month' : 'default_payment_next_month'}, inplace=True)"
      ],
      "metadata": {
        "id": "BAv84W7Ab0OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.info()"
      ],
      "metadata": {
        "id": "UMc-B8M5cDE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit.head()"
      ],
      "metadata": {
        "id": "W3SotA1vcYro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-We can renaming dependendent Variable and renaming some feature name for better understanding of feature"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependent Variable Distribution"
      ],
      "metadata": {
        "id": "f_z_6EY-eRQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_credit['default_payment_next_month'].value_counts()"
      ],
      "metadata": {
        "id": "Twxv1yUUczVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the count plot to vizualize the data distribution \n",
        "#plot the count plot to check the data distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x = 'default_payment_next_month', data = df_credit)"
      ],
      "metadata": {
        "id": "keVO66PDdH87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I picked countplot here as it's easy to compare between default and not default payments using them."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above charts we can observe that exactly 23364 dataset clients are not anticipated to default on payments, whereas  exactly 6636 clients are anticipated to do so."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "According to the insights found we can say that 22% of clients are anticipated to default so it is very important to reduce this percentage to create a positive business impact, this can be done by analysing various other features which affect this as done in further EDA.\n",
        "\n",
        "While the insights themselves may not lead to negative growth, it's important to consider potential pitfalls that could arise if they are not properly interpreted or acted upon."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorical Features\n",
        "We have few categorical features in our dataset that are \\\n",
        "\n",
        "-sex \\\n",
        "-education \\\n",
        "-marraige \\\n",
        "-age \\\n",
        "\n",
        "Categorical variables are qualitative data in which the values are assigned to a set of distinct groups or categories. These groups may consist of alphabetic (e.g., male, female) or numeric labels (e.g., male = 0, female = 1) that do not contain mathematical information beyond the frequency counts related to group membership.\n",
        "\n",
        "Let'Check how they are related with out target class."
      ],
      "metadata": {
        "id": "Wq3XUf63fzrr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SEX\n",
        "\n"
      ],
      "metadata": {
        "id": "Ho9VScvugEF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 - Male\n",
        "\n",
        "2 - Female"
      ],
      "metadata": {
        "id": "aYBPucjwgHpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the count plot to vizualize the data distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x = 'SEX', data = df_credit)"
      ],
      "metadata": {
        "id": "MMOzaKn1f3QF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot is used to represent the occurrence(counts) of the observation present in the categorical variable.\n",
        "\n",
        "It uses the concept of a bar chart for the visual depiction"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that Female credit card holder is more than Male."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, as we can see that Female credit card holder is more than Male so to increase male customers, Bank can give some offers to increase Male customers and at the same time they should take care of their Female customer to increase their business."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlyc3W3uCvYG"
      },
      "source": [
        "# Education\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps2dBdYwCy-a"
      },
      "source": [
        "1 = graduate school; 2 = university; 3 = high school; 0 = others"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXtOLdE4DSYs"
      },
      "outputs": [],
      "source": [
        "# counts the education  data set variable data set\n",
        "df_credit['EDUCATION'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQGlEsFCDS_Q"
      },
      "source": [
        "EDUCATION' column: notice 5 and 6 are both recorded as 'unknown' and there is 0 which isn't explained in the dataset description. Since the amounts are so small, let's combine 0,4,5,6 to 0 which means\"other'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dy9J7rM75Ap"
      },
      "outputs": [],
      "source": [
        "df_credit['EDUCATION'].value_counts()\n",
        "# Change values 4, 5, 6 to 0 and define 0 as 'others'\n",
        "df_credit[\"EDUCATION\"] = df_credit[\"EDUCATION\"].replace({4:0,5:0,6:0})\n",
        "df_credit[\"EDUCATION\"].value_counts()\n",
        "#plotting the count plot to vizualize the data distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x = 'EDUCATION', data = df_credit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot is used to represent the occurrence(counts) of the observation present in the categorical variable.\n",
        "\n",
        "It uses the concept of a bar chart for the visual depiction"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum credit card holders are from university followed by graduate school."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see most credit card holders are from University followed by Graduate school so Bank can target these category people to increase their bussiness and as we know source of income for high school candidates are very less so no need to make more focus on this group of people."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1th_i7FDv7Q"
      },
      "source": [
        "# Marriage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iOtryt4D3fK"
      },
      "source": [
        "1 = married; 2 = single; 3 = others"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHi-1xbY74-L"
      },
      "outputs": [],
      "source": [
        "# counts the education  data set\n",
        "df_credit['MARRIAGE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEXCEUA3747Y"
      },
      "outputs": [],
      "source": [
        "# How many customers had \"MARRIAGE\" status as 0?\n",
        "df_credit[\"MARRIAGE\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLT8MFaC7TGb"
      },
      "source": [
        "'MARRIAGE' column: what does 0 mean in 'MARRIAGE'? Since there are only 0.18% (or 54) observations of 0, we will combine 0 and 3 in one value as 'others'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOQIDWt-7442"
      },
      "outputs": [],
      "source": [
        "# Combine 0 and 3 by changing the value 0 into others\n",
        "df_credit['MARRIAGE'] = df_credit['MARRIAGE'].replace([0], 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfTS5fWV742B"
      },
      "outputs": [],
      "source": [
        "#plotting the count plot to vizualize the data distribution\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.countplot(x = 'MARRIAGE', data = df_credit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot is used to represent the occurrence(counts) of the observation present in the categorical variable.\n",
        "\n",
        "It uses the concept of a bar chart for the visual depiction"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umRltc7Y70ay"
      },
      "source": [
        "From the above data analysis we can say that\n",
        "\n",
        "1 - married\n",
        "\n",
        "2 - single\n",
        "\n",
        "3 - others\n",
        "\n",
        "More number of credit cards holder are Single."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single people make lots of expenditure than married people so they will use credit card more.Hence targetting Single people will surely increase the business."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOuTPAJE72xB"
      },
      "source": [
        "# AGE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZvhryO476YJ"
      },
      "source": [
        "Plotting graph of number of ages of all people with credit card irrespective of gender.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3fKIp5n74zU"
      },
      "outputs": [],
      "source": [
        "# counts the education  data set\n",
        "df_credit['AGE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0F6KS2M74w2"
      },
      "outputs": [],
      "source": [
        "#check the mean of the age group rescpective to the default_payment_next_month\n",
        "df_credit.groupby('default_payment_next_month')['AGE'].mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbjfyQmT74ux"
      },
      "outputs": [],
      "source": [
        "df = df_credit.astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abLqQPLL74sj"
      },
      "outputs": [],
      "source": [
        "#plotting the count plot to vizualize the data distribution\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.style.use('ggplot')\n",
        "sns.countplot(x = 'AGE', data = df_credit)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above data analysis we can say that\n",
        "\n",
        "We can see more number of credit cards holder age are between 26-30 years old. Age above 60 years old rarely uses the credit card."
      ],
      "metadata": {
        "id": "Nu0cUztBhdjd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqZQ81yU74p6"
      },
      "outputs": [],
      "source": [
        "#plotting the box plot to vizualize the data distribution\n",
        "plt.style.use('ggplot')\n",
        "plt.figure(figsize=(10,10))\n",
        "ax = sns.boxplot(x=\"default_payment_next_month\", y=\"AGE\", data=df_credit, palette=['c', 'm'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot is used to represent the occurrence(counts) of the observation present in the categorical variable.\n",
        "\n",
        "It uses the concept of a bar chart for the visual depiction."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that most of the credit card holders age range between 26-30 years.\n",
        "\n",
        "Credit card holders are very less after age of 60 years."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Youngsters are using credit card more so we will mainly focus on them to increase our business.\n",
        "\n"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b68INi6tDH3m"
      },
      "source": [
        "# Numerical features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ypCvqPODGkO"
      },
      "source": [
        "Limit Balance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mLU1rI9v74ml"
      },
      "outputs": [],
      "source": [
        "# describe  the limit balance  data set\n",
        "df_credit['LIMIT_BAL'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZUYsUPK74jl"
      },
      "outputs": [],
      "source": [
        "#plotting the dist plot to vizualize the data distribution\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.distplot(df_credit['LIMIT_BAL'], kde=True, color='r')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBU1UB3MGHCv"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with a size of 15x10 inches\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# Plot a histogram of the 'LIMIT_BAL' column from the df_credit dataframe\n",
        "plt.hist(df_credit['LIMIT_BAL'], bins=20, color='r', alpha=0.5)\n",
        "\n",
        "# Add a title and axis labels\n",
        "plt.title('Distribution of Credit Limit')\n",
        "plt.xlabel('Credit Limit')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Show the resulting plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qTWuboOEGR2"
      },
      "outputs": [],
      "source": [
        "#plotting the bar plot to vizualize the data distribution\n",
        "sns.barplot(x='default_payment_next_month', y='LIMIT_BAL', data=df_credit, palette=['r','m'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3nvU49sF7OX"
      },
      "outputs": [],
      "source": [
        "#plotting the box plot to vizualize the data distribution\n",
        "plt.figure(figsize=(10,10))\n",
        "ax = sns.boxplot(x=\"default_payment_next_month\", y=\"LIMIT_BAL\", data=df_credit, palette=['m','c'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Density Plot visualises the distribution of data over a continuous interval or time period. This chart is a variation of a Histogram that uses kernel smoothing to plot values, allowing for smoother distributions by smoothing out the noise.\n",
        "\n",
        "Here i am using a countplot so that i can get counts for different classes that my dataset contain and can visually interpret the results for different classes.\n",
        "\n",
        "Box plots are used to show distributions of numeric data values, especially when you want to compare them between multiple groups. They are built to provide high-level information at a glance, offering general information about a group of data’s symmetry, skew, variance, and outliers."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the countplot we can see the various count of credit limit and when combing with distribution plot we can see that it is positively skewed data and from the boxplot we can infer that there are many outliers in credit limit."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRNNcUXlHWn3"
      },
      "source": [
        "# Total Bill Amount"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYLXJ3EKG8g9"
      },
      "outputs": [],
      "source": [
        "#assign the bill amount variable to a single variable \n",
        "total_bill_amount =df[['BILL_AMT_SEPT',\t'BILL_AMT_AUG',\t'BILL_AMT_JUL',\t'BILL_AMT_JUN',\t'BILL_AMT_MAY',\t'BILL_AMT_APR']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-mKSHp5H7pr"
      },
      "outputs": [],
      "source": [
        "#plotting the pair plot for bill amount \n",
        "sns.pairplot(data = total_bill_amount)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. Pairplot visualizes given data to find the relationship between them where the variables can be continuous or categorical.\n",
        "\n",
        "So, I have used pair plot to represent the data in graphical form and to see any relationship in between the various features."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of the bill amounts and pay amounts are right skewed. Bill amounts are aproximately linearly related."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf-2ZoMeJaCM"
      },
      "source": [
        "# History payment status"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJR8xS1aJKTt"
      },
      "outputs": [],
      "source": [
        "pay_col = ['PAY_SEPT',\t'PAY_AUG',\t'PAY_JUL',\t'PAY_JUN',\t'PAY_MAY',\t'PAY_APR']\n",
        "for col in pay_col:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  sns.countplot(x = col, hue = 'default_payment_next_month', data = df_credit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaSYAmb0Szig"
      },
      "outputs": [],
      "source": [
        "pay_col = ['PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR']\n",
        "\n",
        "# Create a grid of subplots with 2 columns and 3 rows\n",
        "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
        "\n",
        "# Plot a countplot for each payment column in a subplot\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    if i < len(pay_col):\n",
        "        sns.countplot(x=pay_col[i], hue='default_payment_next_month', data=df_credit, ax=ax, palette='Set2')\n",
        "\n",
        "# Adjust the space between the subplots and show the plot\n",
        "plt.tight_layout()\n",
        "fig.suptitle('Count of Payment Status by Default Payment Next Month', y=1.05)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above figure shows bar plot for each month payment status which show the count of defaulters and non-defaulter."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of defaulters are decreasing as we move on the x-scale which is good for he companies business."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMkXj0kTTQ22"
      },
      "source": [
        "# Paid Amount"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code "
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUMG5KL8TR9g"
      },
      "outputs": [],
      "source": [
        "#assign the Paid Amount variable to a single variable \n",
        "pay_amnt_df = df[['PAY_AMT_SEPT',\t'PAY_AMT_AUG',\t'PAY_AMT_JUL',\t'PAY_AMT_JUN',\t'PAY_AMT_MAY',\t'PAY_AMT_APR', 'default_payment_next_month']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gu9uVwD5VBlQ"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(data=pay_amnt_df, hue='default_payment_next_month', kind='reg')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a pair-plot we aim to visualize the correlation of each feature pair in a dataset against the class distribution at a one glance.Since a pair plot visually gives an idea of correlation of each feature pair, it helps us to understand and quickly analyse the correlation matrix (Pearson) of the dataset as well."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From this we can can visualize that amount of previous payments by defaulters is way less than the non defaulters."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atr3XGOzs71E"
      },
      "source": [
        "# Bivariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code "
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lb2-aI2TtANS"
      },
      "source": [
        "Sex Vs Default Payment Next Month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3WvQkDpWzJX"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code\n",
        "#plotting the cat plot to vizualize the data distribution related to the default_payment_next_month\n",
        "x,y = 'SEX', 'default_payment_next_month'\n",
        "\n",
        "(df_credit\n",
        ".groupby(x)[y]\n",
        ".value_counts(normalize=True)\n",
        ".mul(100)\n",
        ".rename('percent')\n",
        ".reset_index()\n",
        ".pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV7eAZKDuW8B"
      },
      "source": [
        "Education and default_payment_next_month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUWFl3gfuLCZ"
      },
      "outputs": [],
      "source": [
        "#plotting the cat plot to vizualize the data distribution related to the default_payment_next_month\n",
        "x = 'EDUCATION' \n",
        "y = 'default_payment_next_month'\n",
        "df_percent = (df_credit.groupby(x)[y].value_counts(normalize = True).mul(100).rename('percent').reset_index())\n",
        "sns.catplot(data=df_percent, x=x, y='percent', hue=y, kind='bar', palette=['#191825', '#865DFF'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR8wwEtYuoGw"
      },
      "source": [
        "Marriage Vs Default Payment Next Mont"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbfDS-NFuhHq"
      },
      "outputs": [],
      "source": [
        "#plotting the cat plot to vizualize the data distribution related to the default_payment_next_month\n",
        "x = 'EDUCATION' \n",
        "y = 'default_payment_next_month'\n",
        "df_percent = (df_credit.groupby(x)[y].value_counts(normalize = True).mul(100).rename('percent').reset_index())\n",
        "sns.catplot(data=df_percent, x=x, y='percent', hue=y, kind='bar', palette=['#191825', '#865DFF'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grfrHdPpwrP6"
      },
      "source": [
        "Age Vs Default Payment Next Month\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFL41kRgwr26"
      },
      "outputs": [],
      "source": [
        "#plotting the bar plot to vizualize the data distribution related to the default_payment_next_month\n",
        "plt.figure(figsize=(19,7))\n",
        "sns.barplot(x = 'AGE', y = 'default_payment_next_month', data = df_credit)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The countplot is used to represent the occurrence(counts) of the observation present in the categorical variable.\n",
        "\n",
        "It uses the concept of a bar chart for the visual depiction.\n",
        "\n"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sex - \n",
        "As we know maximum number of credit card holders are Female and from above graph it is also clear that maximum number of defaulters are Female.\n",
        "\n",
        "In terms of default ratio(default/(default+not_default)),males having more default ratio than females.\n",
        "\n",
        "Education - \n",
        "As maximum number of credit card holder is from unversity and from above graph it is clear that more number of defaulter is also from University only but again in terms of default ratio university people having less default ratio.\n",
        "\n",
        "Marriage - \n",
        "From above we can see that married people having higher default ratio\n",
        "\n",
        "Age - \n",
        "From 21 t early 60s default ratio is varying non-linearly however after 60s default ratio is getting increased.\n"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sex -\n",
        " Bank shall mainly focus on Female customers to increase the bussiness.However some policies should be provided to Male customers also to reduce the chance of being default.\n",
        "\n",
        "Education- \n",
        "We can target on University people more to increase our bussiness as most of the credit card holder is from this category.\n",
        "\n",
        "Marriage - \n",
        "Married people having high default ratio than single and as we can see that most of the non-defaulter/maximum credit card holders belongs to Single category so we can mainly focus on them to increase the business.\n",
        "\n",
        "Age - \n",
        "For the age of 21s to early 60s there is almost constant proportion of age default for credit card payment yet one insightful information is that there is higher risk for people of age group more than 60s."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code "
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxHK1E1Yw0Jr"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "#plotting the heatmap \n",
        "plt.figure(figsize=(20,15))\n",
        "sns.heatmap(df_credit.corr(),annot=True,cmap=\"coolwarm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_zVVR5bxFsI"
      },
      "source": [
        "It seems from the above graph is there are some negatively correlated feature like age but we cannot blindly remove this feature because it could be important feature for prediction\\\n",
        "ID is unimportant and it has no role in prediction so we will remove it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwceAagptW6R"
      },
      "outputs": [],
      "source": [
        "# Draw box plot to see if there is any outliers in our dataset\n",
        "plt.figure (figsize= (18,7))\n",
        "df_credit.boxplot()\n",
        "plt.xticks(rotation=90)\n",
        "# rotating xticks to 90 degrees. this is done when we want our x-axis label annotators to be vertical \n",
        "# because there may not be enough space for us to visualize them. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KCYO6lXtncP"
      },
      "source": [
        "From the above boxplot, we can see that there are quite a few outliers present in our features. And most of these outliers are present in features containing pay-amount and Bill amount data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iftVxN9ctWzF"
      },
      "outputs": [],
      "source": [
        "# creating a list columns in which outliers are present.\n",
        "outlier_columns = ['LIMIT_BAL', 'BILL_AMT_SEPT','BILL_AMT_AUG', 'BILL_AMT_JUL', 'BILL_AMT_JUN', 'BILL_AMT_MAY',\n",
        "                 'BILL_AMT_APR', 'PAY_AMT_SEPT', 'PAY_AMT_AUG', 'PAY_AMT_JUL','PAY_AMT_JUN', 'PAY_AMT_MAY',\n",
        "                 'PAY_AMT_APR']\n",
        "# using IQR method for dropping outliers from above columns\n",
        "Q1 = df[outlier_columns].quantile(0.25)\n",
        "Q3 = df[outlier_columns].quantile(0.75)\n",
        "\n",
        "IQR = Q3 - Q1                   # interquartile range\n",
        "\n",
        "# using interquartile range to find and remove outliers from our dataframe.\n",
        "df = df[~((df[outlier_columns] < (Q1 - 1.5 * IQR)) |(df[outlier_columns] > (Q3 + 1.5 * IQR))).any(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z3xwj-etWwY"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CTYSR0TtWdY"
      },
      "outputs": [],
      "source": [
        "# Dropping some of the unnecessary columns.\n",
        "df.drop(['ID'], axis=1,inplace =True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSN-Fmj_u2sC"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation heatmaps are a type of plot that visualize the strength of relationships between numerical variables.\n",
        "\n",
        "Correlation plots are used to understand which variables are related to each other and the strength of this relationship.\n",
        "\n",
        "A correlation plot typically contains a number of numerical variables, with each variable represented by a column. The rows represent the relationship between each pair of variables.\n",
        "\n",
        "The values in the cells indicate the strength of the relationship, with positive values indicating a positive relationship and negative values indicating a negative relationship."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems from the above graph is there are some negatively correlated feature like age but we cannot blindly remove this feature because it could be important feature for prediction.\n",
        "\n",
        "ID is unimportant and it has no role in prediction so we will remove it."
      ],
      "metadata": {
        "id": "9hKA6lWu70Tu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAdRQQcqvlNh"
      },
      "source": [
        "## ***. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHMpN4NbvhTu"
      },
      "outputs": [],
      "source": [
        "# Now checking for correlation among our dependent variables (Multicollinearity) using VIF analysis.\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX0Y3Jubv2Qp"
      },
      "outputs": [],
      "source": [
        "# performing VIF analysis\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['is_defaulter']]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4ngq13nwE04"
      },
      "source": [
        "As we can see from above, that some of our features have high multicollinearity in them particularly the bill amount columns. so we need to do some feature engineering on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZdGSUYrwHuq"
      },
      "outputs": [],
      "source": [
        "# Lets add up all bill amount features together in one.\n",
        "df['TOTAL_BILL_PAY'] = df['BILL_AMT_SEPT'] + df['BILL_AMT_AUG'] + df['BILL_AMT_JUL'] + df['BILL_AMT_JUN'] +  df['BILL_AMT_MAY'] + df['BILL_AMT_APR'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX7-Ru6ZwUdW"
      },
      "outputs": [],
      "source": [
        "# Lets check again.\n",
        "calc_vif(df[[i for i in df.describe().columns if i not in ['is_defaulter','BILL_AMT_SEPT','BILL_AMT_AUG','BILL_AMT_JUL','BILL_AMT_JUN','BILL_AMT_MAY','BILL_AMT_APR']]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Categorical Encoding"
      ],
      "metadata": {
        "id": "4x6gIYCjEOi0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One hot encoding "
      ],
      "metadata": {
        "id": "84zYCgpmfpTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#assigning the value for diffrent categories\n",
        "df.replace({'SEX': {1 : 'MALE', 2 : 'FEMALE'}, 'EDUCATION' : {1 : 'graduate school', 2 : 'university', 3 : 'high school', 0 : 'others'}, 'MARRIAGE' : {1 : 'married', 2 : 'single', 3 : 'others'}}, inplace = True)\n"
      ],
      "metadata": {
        "id": "V3bymPGcf4TD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "mwZqnbNBgzbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dummy variables\n",
        "df = pd.get_dummies(df, columns = ['EDUCATION', 'MARRIAGE'])"
      ],
      "metadata": {
        "id": "OehUVX32g4MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "BwzXnsxdg4JV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating dummy variables by droping first variable\n",
        "df = pd.get_dummies(df, columns=['PAY_SEPT', 'PAY_AUG', 'PAY_JUL', 'PAY_JUN', 'PAY_MAY', 'PAY_APR'])"
      ],
      "metadata": {
        "id": "KU9BNbuHg4Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LABEL ENCODING FOR Gender\n",
        "encoders_nums = {\"SEX\":{\"FEMALE\": 0, \"MALE\": 1}}\n",
        "df = df.replace(encoders_nums)"
      ],
      "metadata": {
        "id": "HV_uEkDzg4B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "f3mDiM-KhVqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "xODgjKElhVh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "tZk3-LZGjX_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating dependent variable and independent variable\n",
        "independent_variables = df.drop(['default_payment_next_month'],axis=1)\n",
        "dependent_variable = df['default_payment_next_month']"
      ],
      "metadata": {
        "id": "9xptFmOFifvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Scaling"
      ],
      "metadata": {
        "id": "3bPArefLWz_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling the data using zscore.\n",
        "from scipy.stats import zscore  \n",
        "x = round(independent_variables.apply(zscore),3)\n",
        "y = dependent_variable"
      ],
      "metadata": {
        "id": "SkvaFwnPifs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data spliting"
      ],
      "metadata": {
        "id": "XW1-Jbv_WV3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 1)"
      ],
      "metadata": {
        "id": "ee5b8CG_ifqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "2f-zMC8aEVe0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One-hot encoding is an important step for preparing your dataset for use in machine learning. One-hot encoding turns your categorical data into a binary vector representation. Pandas get dummies makes this very easy.\n",
        "This is important when working with many machine learning algorithms, such as decision trees and support vector machines, which accept only numeric inputs.\n",
        "This means that for each unique value in a column, a new column is created. The values in this column are represented as 1s and 0s, depending on whether the value matches the column header."
      ],
      "metadata": {
        "id": "njhAcDp0EX9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "qHG5eIOaFDj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Z-score Scaler to scale my data because  they give us a sense of where a score falls in relation to the mean of its population (in terms of standard deviation of its population), they allow us to compare scores from different distributions, and  they can be transformed into percentiles."
      ],
      "metadata": {
        "id": "8cPIZ4qyFHC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "Q5zIGBoFGJgD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I haved used 80/20 split ratio for train and test dataset as my model will get enough data to train itself and after that we can test our model on the unseen data."
      ],
      "metadata": {
        "id": "WuXqBtqwGOBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "L574qEQuG02q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# APPLYING SMOTE (Synthetic Minority Oversampling Technique)"
      ],
      "metadata": {
        "id": "vglLt_8-lvnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we have an imbalanced dataset, we are going to need to apply some technique to remedy this. So we will try oversampling technique called SMOTE."
      ],
      "metadata": {
        "id": "8J0a3JiAlzdb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# applying oversampling to overcome class imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote= SMOTE()\n",
        "x_train_smote,y_train_smote = smote.fit_resample(x,y)\n",
        "\n",
        "from collections import Counter\n",
        "print('Original dataset shape', Counter(y_train))\n",
        "print('Resample dataset shape', Counter(y_train_smote))\n",
        "Counter(y_train_smote)"
      ],
      "metadata": {
        "id": "JjrI17atifnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "CcKI8ZAfG02y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes this an imbalanced dataset as our target variable has a very high number of non defaulters compared to defaulters and this is good but when we are using machine learning models we need to care of this imbalance using various technique such SMOTE,Tomek links,Undersampling,Oversampling beacause if dont take care of this our overall accuracy would be high simply because the most transaction is not defaulters(not because your model is any good) and we will not be able to distinguish between defaulters and non defaulters and this will impact the business negatively.\n",
        "\n",
        "As we have seen earlier that we have imbalanced dataset. So to remediate Imbalance we are using SMOTE(Synthetic Minority Oversampling Technique\n",
        "\n",
        "SMOTE (Synthetic Minority Oversampling Technique) – Oversampling is one of the most commonly used oversampling methods to solve the imbalance problem. It aims to balance class distribution by randomly increasing minority class examples by replicating them."
      ],
      "metadata": {
        "id": "FNLZgJkFHAFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  ML Model Implementation"
      ],
      "metadata": {
        "id": "IQGOmZxtGfqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing all the evaluation metrics that we will need for comparison.\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, auc, classification_report"
      ],
      "metadata": {
        "id": "8PgBoXCsl3mI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "_a7cgIJDoh0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Logistics Regression and GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "metadata": {
        "id": "zUkqC0uvl3hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initiate the model.\n",
        "logistic_model = LogisticRegression(class_weight='balanced')\n",
        "\n",
        "# define the parameter grid.\n",
        "param_grid = {'penalty':['l1','l2'], 'C' : [0.0001,0.001,0.003,0.004,0.005, 0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 10, 20, 50, 100] }\n",
        "\n",
        "# implementing the model.\n",
        "logistic_model= GridSearchCV(logistic_model, param_grid, scoring = 'accuracy', n_jobs = -1, verbose = 3, cv = 3)\n",
        "logistic_model.fit(x_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "0gP-vIUkl3cX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best estimator\n",
        "logistic_model.best_estimator_"
      ],
      "metadata": {
        "id": "jXxX4mqpl3Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the optimal parameters\n",
        "logistic_model.best_params_ "
      ],
      "metadata": {
        "id": "p7YlL5yIl3S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the predicted probability of target variable.\n",
        "y_train_preds_logistic = logistic_model.predict_proba(x_train_smote)[:,1]\n",
        "y_test_preds_logistic = logistic_model.predict_proba(x_test)[:,1]"
      ],
      "metadata": {
        "id": "QVjIZDQLl3N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the predicted class\n",
        "y_train_class_preds_logistic = logistic_model.predict(x_train_smote)\n",
        "y_test_class_preds_logistic = logistic_model.predict(x_test)"
      ],
      "metadata": {
        "id": "vu9EIwRKpAxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the accuracy on training and unseen test data.\n",
        "logistic_train_accuracy= accuracy_score(y_train_smote, y_train_class_preds_logistic)\n",
        "logistic_test_accuracy= accuracy_score(y_test, y_test_class_preds_logistic)\n",
        "\n",
        "print(\"The accuracy on train data is \", logistic_train_accuracy)\n",
        "print(\"The accuracy on test data is \", logistic_test_accuracy)"
      ],
      "metadata": {
        "id": "rkzGxlzepArm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# writing a function for evaluating various metrics\n",
        "def evaluation_metrics(actual, predicted):\n",
        "\n",
        "  \"\"\" This function is used to find the accuracy score , precision score , recall score , f1 score , ROC_AUC Score , \n",
        "      Confusion Matrix , Classification  report \"\"\"\n",
        "  metrics_list = []\n",
        "  accuracy = accuracy_score(actual,predicted)\n",
        "  precision = precision_score(actual, predicted)\n",
        "  recall = recall_score(actual, predicted)\n",
        "  model_f1_score = f1_score(actual, predicted)\n",
        "  auc_roc_score = roc_auc_score(actual , predicted)\n",
        "  model_confusion_matrix = confusion_matrix(actual , predicted)\n",
        "\n",
        "  metrics_list = [accuracy,precision,recall,model_f1_score,auc_roc_score, model_confusion_matrix]\n",
        "  return metrics_list"
      ],
      "metadata": {
        "id": "tqGpXsiMpAl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metrics(y_test, y_test_class_preds_logistic)"
      ],
      "metadata": {
        "id": "urJIoCtYpAgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's store these metrics in a dataframe. that way we can easily compare metrics of different models.\n",
        "# first store this data in a dict.\n",
        "metric_name_list = ['accuracy','precision','recall','f1_score','roc_auc_score','confusion_matrix']\n",
        "metric_values = evaluation_metrics(y_test, y_test_class_preds_logistic)\n",
        "\n",
        "# zipping together above lists to form a dictionary\n",
        "metric_dict = dict(zip(metric_name_list,metric_values))\n",
        "\n",
        "# creating a dataframe out of this. \n",
        "evaluation_metric_df = pd.DataFrame.from_dict(metric_dict, orient='index').reset_index()\n",
        "evaluation_metric_df.columns = ['Evaluation Metric','Logistic Regression']"
      ],
      "metadata": {
        "id": "lBds-h3ApPgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "Z8oVr3wmpUIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the confusion matrix from test data\n",
        "\n",
        "labels = ['Non Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_test,y_test_class_preds_logistic)\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, cmap='coolwarm', ax = ax, lw = 3) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('Actual labels')\n",
        "ax.set_title('Confusion Matrix of Logistics Regression from testing data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "\n",
        "# also printing confusion matrix values\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "oVHLhEU5pbYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Roc_auc_curve for test data\n",
        "y_test_pred_logistic = logistic_model.predict_proba(x_test)[:,1]\n",
        "fpr, tpr, _ = roc_curve(y_test,y_test_pred_logistic)\n",
        "plt.plot(fpr,tpr)\n",
        "plt.title(\"Roc_auc_curve on Test data\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I6sqNyELpkZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# printing the classification report.\n",
        "print('classification_report is \\n {}'.format(classification_report(y_test, y_test_class_preds_logistic)))"
      ],
      "metadata": {
        "id": "sDpaAFpVpp7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "wHElkbgIpvXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "6ursfgJZI10b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this case is Logistic Regression, which is a popular supervised learning algorithm used for binary classification tasks. It models the relationship between the input features and the probability of a binary outcome.\n",
        "\n",
        "Logistic regression is an excellent tool to know for classification problems, which are problems where the output value that we wish to predict only takes on only a small number of discrete values. Here we'll focus on the binary classification problem, where the output can take on only two distinct classes.\n",
        "\n",
        "the Logistic Regression model achieved an accuracy of 0.74943, indicating a reasonably good overall performance. However, the precision and recall scores are relatively low, suggesting that the model struggles with correctly identifying positive instances. The F1-score, which combines precision and recall, also falls in the moderate range. The AUC-ROC score indicates a decent discrimination capability of the model. The confusion matrix provides a detailed breakdown of the model's predictions, showing its strengths and weaknesses in differentiating between positive and negative instances."
      ],
      "metadata": {
        "id": "dKRjxAeqI5Vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "W6fSI4mmL5eL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Grid Search CV In Grid Search, we try every combination of a preset list of values of the hyper-parameters and choose the best combination based on the cross-validation score.\n",
        "\n"
      ],
      "metadata": {
        "id": "IlMzbdydL7a9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Random Forest Classifier\n",
        "\n"
      ],
      "metadata": {
        "id": "Yo8aRyEMH9KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Random forest \n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "5e28wEHLpvQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rf= RandomForestClassifier()                                                              # initializing the model.\n",
        "\n",
        "grid_values = {'n_estimators':[50,80,90,100], 'max_depth':[9,11,14]}              # initializing the parameter grid.\n",
        "grid_rf = GridSearchCV(model_rf, param_grid = grid_values, scoring = 'accuracy', cv=3)\n",
        "\n",
        "# Fitting the model.\n",
        "grid_rf.fit(x_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "FQpVu1hkpvKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the best parameter\n",
        "grid_rf.best_params_ "
      ],
      "metadata": {
        "id": "7nLaoiKvpvEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the predicted classes\n",
        "y_train_class_preds_rf = grid_rf.predict(x_train_smote)\n",
        "y_test_class_preds_rf = grid_rf.predict(x_test)"
      ],
      "metadata": {
        "id": "nZaJ7DCcpu9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the evaluation metrics using our function and adding it to evaluation dataframe to better read it.\n",
        "evaluation_metric_df['Random Forest']=evaluation_metrics(y_test,y_test_class_preds_rf)\n",
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "gqwFEY-upu3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the confusion matrix from test data\n",
        "\n",
        "labels = ['Non Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_test,y_test_class_preds_rf)\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, cmap='coolwarm', ax = ax, lw = 3) #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('Actual labels')\n",
        "ax.set_title('Confusion Matrix of Random Forest from testing data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)\n",
        "\n",
        "# also printing confusion matrix values\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "ijxXxiNvpuxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('classification_report is \\n {}'.format(classification_report(y_test, y_test_class_preds_rf)))"
      ],
      "metadata": {
        "id": "5kbEevcBpurV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing Roc_auc_curve from test data\n",
        "\n",
        "y_test_preds_proba_rf = grid_rf.predict_proba(x_test)[::,1]\n",
        "fpr, tpr, _ = roc_curve(y_test,  y_test_preds_proba_rf)\n",
        "auc = roc_auc_score(y_test,  y_test_preds_proba_rf)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.title(\"Roc_auc_curve on testing data\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J1lbYM5hpulD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest model has inbuilt support for showing the feature importances - i.e. which feature is more important in coming up with the predicted results. This helps us interpret and understand the model better."
      ],
      "metadata": {
        "id": "kJEydjHKseQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# getting columns names from training data\n",
        "features = x_train_smote.columns\n",
        "\n",
        "# getting the feature importances\n",
        "importances = grid_rf.best_estimator_.feature_importances_\n",
        "indices = np.argsort(importances)"
      ],
      "metadata": {
        "id": "Ar0j0EfTsanF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the feature importances using a horizontal bar graph.\n",
        "plt.figure (figsize= (12,12))\n",
        "plt.title('Relative Feature Importance', fontsize=14)\n",
        "plt.barh(range(len(indices)), importances[indices], color='magenta', edgecolor='mediumblue', align='center')\n",
        "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "plt.ylabel('Features', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U6h1n32Msaf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "WFE139zRMaIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this case is Random Forest, which is an ensemble learning method that combines multiple decision trees to make predictions. Random Forest is a versatile and powerful algorithm that can be used for both classification and regression tasks.\n",
        "\n",
        "The Random Forest model achieved a high accuracy of 0.867748, indicating a strong overall performance. The precision and recall scores are also relatively high, indicating the model's ability to correctly identify positive instances while minimizing false positives. The F1-score provides a balanced assessment of precision and recall. The AUC-ROC score of 0.828852 suggests a good discrimination capability of the model. The confusion matrix provides a detailed breakdown of the model's predictions, showing its strengths and weaknesses in differentiating between positive and negative instances."
      ],
      "metadata": {
        "id": "SDSX-otUMkBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "KUYOL456O0xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Grid Search CV In Grid Search, we try every combination of a preset list of values of the hyper-parameters and choose the best combination based on the cross-validation score.\n",
        "\n"
      ],
      "metadata": {
        "id": "VTO1tHHVO0xW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. K-Nearest Neighbour Classifier"
      ],
      "metadata": {
        "id": "imd-Jsn6spph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import K Nearest Neighbour Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "metadata": {
        "id": "42ToRFLvspOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing the model\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# knn the parameter to be tuned is n_neighbors\n",
        "param_grid = {'n_neighbors':[4,5,6,7,8,10,12,14]}\n",
        "\n",
        "# Fitting the model\n",
        "\n",
        "knn_cv= GridSearchCV(knn,param_grid, scoring = 'accuracy',cv=3)\n",
        "knn_cv.fit(x_train_smote,y_train_smote)"
      ],
      "metadata": {
        "id": "FT-mLEOEsaYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find best score \n",
        "knn_cv.best_score_"
      ],
      "metadata": {
        "id": "Js9x6L61saRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# best parameters\n",
        "knn_cv.best_params_"
      ],
      "metadata": {
        "id": "TDrwuxRotSfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "knn_cv.best_estimator_"
      ],
      "metadata": {
        "id": "46yukaKEtSXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "y_train_class_preds_knn = knn_cv.predict(x_train_smote)\n",
        "y_test_class_preds_knn = knn_cv.predict(x_test)"
      ],
      "metadata": {
        "id": "ymbHej1PtSPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# getting the evaluation metrics and adding it to metric dataframe. \n",
        "evaluation_metric_df['KNeighborsClassifier'] = evaluation_metrics(y_test,y_test_class_preds_knn)\n",
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "ye6TFiLrtSHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the classification report.\n",
        "print('classification_report is \\n {}'.format(classification_report(y_test, y_test_class_preds_knn)))"
      ],
      "metadata": {
        "id": "ELdvJVW1tR_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the confusion matrix for testing data \n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_test,y_test_class_preds_knn)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, linewidths=1, cmap='coolwarm',ax = ax) \n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix of KNN Classifier for testing data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "GqO0Cd9PtpZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing Roc_auc_curve from test data\n",
        "\n",
        "y_test_preds_proba_knn = knn_cv.predict_proba(x_test)[::,1]\n",
        "fpr, tpr, _ = roc_curve(y_test,  y_test_preds_proba_knn)\n",
        "auc = roc_auc_score(y_test,  y_test_preds_proba_rf)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.title(\"Roc_auc_curve on testing data\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ypK_OAbNtvWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "rKs4xez8O69S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The k-nearest neighbors algorithm, also known as KNN or k-NN, is a non-parametric, supervised learning classifier, which uses proximity to make classifications or predictions about the grouping of an individual data point. While it can be used for either regression or classification problems, it is typically used as a classification algorithm, working off the assumption that similar points can be found near one another.\n",
        "\n",
        "The K-Nearest Neighbors (KNN) Classifier model achieved a good accuracy of 0.85964, indicating a relatively strong overall performance. The precision and recall scores are reasonably good, indicating the model's ability to correctly identify positive instances while minimizing false positives. The F1-score provides a balanced assessment of precision and recall. The AUC-ROC score of 0.854637 suggests a good discrimination capability of the model. The confusion matrix provides a detailed breakdown of the model's predictions, showing its strengths and weaknesses in differentiating between positive and negative instances."
      ],
      "metadata": {
        "id": "AHrD00lEO9tg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "Fqio_TVQQOY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Grid Search CV In Grid Search, we try every combination of a preset list of values of the hyper-parameters and choose the best combination based on the cross-validation score.\n",
        "\n"
      ],
      "metadata": {
        "id": "KlB9g7QlQOY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Support Vector Classifier"
      ],
      "metadata": {
        "id": "umNF6zw9Je-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing support vector machine algorithm from sklearn\n",
        "from sklearn import svm\n",
        " \n",
        "# initiate a svm Classifier\n",
        "svm_model = svm.SVC(kernel = 'poly',gamma='scale', probability=True)\n",
        "\n",
        "# fit the model using the training sets\n",
        "svm_model.fit(x_train_smote, y_train_smote)"
      ],
      "metadata": {
        "id": "ElIygQKRtvN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted classes\n",
        "y_train_class_preds_svm = svm_model.predict(x_train_smote)\n",
        "y_test_class_preds_svm = svm_model.predict(x_test)"
      ],
      "metadata": {
        "id": "UBbnju9GxRHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_metric_df['Support Vector classifier'] = evaluation_metrics(y_test,y_test_class_preds_svm)\n",
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "7QuawBowtvFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the confusion matrix for testing data \n",
        "labels = ['Not Defaulter', 'Defaulter']\n",
        "cm = confusion_matrix(y_test,y_test_class_preds_svm)\n",
        "print(cm)\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, linewidths=1, cmap='coolwarm',ax = ax) \n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix of SVM Classifier for testing data')\n",
        "ax.xaxis.set_ticklabels(labels)\n",
        "ax.yaxis.set_ticklabels(labels)"
      ],
      "metadata": {
        "id": "g63J-16btu0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the classification report.\n",
        "print('classification_report is \\n {}'.format(classification_report(y_test, y_test_class_preds_knn)))"
      ],
      "metadata": {
        "id": "ya7VjAoRxxZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Roc_auc_curve on taining data\n",
        "\n",
        "y_train_preds_proba_svm = svm_model.predict_proba(x_train_smote)[::,1]\n",
        "fpr, tpr, _ = roc_curve(y_train_smote,  y_train_preds_proba_svm )\n",
        "auc = roc_auc_score(y_train_smote,  y_train_preds_proba_svm )\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.title(\"Roc_auc_curve on Training data\")\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rp7JEEWUxxPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# finally, we can compare our models on variour evaluation metric values.\n",
        "evaluation_metric_df"
      ],
      "metadata": {
        "id": "BP6XR2BuxxGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "-A0di-dbQTdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support Vector Machines (SVMs in short) are machine learning algorithms that are used for classification and regression purposes. SVMs are one of the powerful machine learning algorithms for classification, regression and outlier detection purposes. An SVM classifier builds a model that assigns new data points to one of the given categories. Thus, it can be viewed as a non-probabilistic binary linear classifier.\n",
        "\n",
        "SVMs can be used for linear classification purposes. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using the kernel trick. It enable us to implicitly map the inputs into high dimensional feature spaces.\n",
        "\n",
        "the Support Vector Classifier (SVC) model achieved a moderate accuracy of 0.777046, indicating a reasonable overall performance. The precision and recall scores are relatively low, suggesting that the model struggles with correctly identifying positive instances. The F1-score provides a balanced assessment of precision and recall. The AUC-ROC score of 0.717235 suggests a moderate discrimination capability of the model. The confusion matrix provides a detailed breakdown of the model's predictions, showing its strengths and weaknesses in differentiating between positive and negative instances."
      ],
      "metadata": {
        "id": "JHQgaDiaQVsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The important metric to compare all the algorithms in this case is ‘Recall’. As the company can’t afford to predict False negative i.e. predict defaulter as a non defaulter. Since, company is one, who will give to money to the customers,if, for any reason giving money to defaulter is gaining more risk to getting the investment back. Hence, here identifying false negative is important."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will choose K-Nearest Neghbors as my final prediction model because Recall for KNN model  aprrox 84% which highigher than other models ."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**After conducting this thorough exercise, we found that :**\n",
        "\n",
        "- Most of the credit card users are Female and have higher number of defaults.\n",
        "Most of the credit card users are highly educated.\n",
        "Single users have more no. of credit cards.\n",
        "\n",
        "- The number of credit card users goes down with increase in age as old people have less consumption and may not be able to use credit cards and their purchases are usually made by younger family members.\n",
        "\n",
        "- Using a Logistic Regression classifier, we can predict an approximate accuracy of 74% and ROC_AUC score of 0.704\n",
        "\n",
        "- Using Random Forest Classifier, we can predict an accuracy of around 86% and ROC_AUC score of 0.82\n",
        "\n",
        "- Using K-Neighbor Classifier, we can predict an accuracy of 85% and ROC_AUC score of 0.854\n",
        "\n",
        "- Using Support Vector Machine Classifier, we can predict an accuracy of 77% and ROC_AUC score of around 0.712\n",
        "\n",
        "- Random Forest Classifier and K Neighbors classifier perform the best among all models.\n",
        "\n",
        "\n",
        "Our best models are Random Forest and K-Neighbor Classifier as they have the best Precision, Recall, ROC_AUC and F1 score values. This being an imbalanced dataset, Recall will be most important metric as we don't want to classify a defaulter as a non defaulter so that makes K Neighbor Classifier model more suitable for the task."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}